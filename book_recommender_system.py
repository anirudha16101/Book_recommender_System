# -*- coding: utf-8 -*-
"""Book_Recommender_System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vqn97JZfv7Tm8TbODxeppjSmgmefLEMZ
"""

from google.colab import files
uploaded=files.upload()

import pandas as pd 
import matplotlib as plt 
import os, sys
import re
import numpy as np 
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import correlation
import ipywidgets as widgets
from IPython.display import display,clear_output
from contextlib import contextmanager
import warnings
warnings.filterwarnings('ignore')
import re 
import seaborn as sns

import sklearn.metrics as metrics
from sklearn.metrics.pairwise import pairwise_distances
from IPython.display import HTML
HTML('''<script>
code_show_err=false; 
function code_toggle_err() {
 if (code_show_err){
 $('div.output_stderr').hide();
 } else {
 $('div.output_stderr').show();
 }
 code_show_err = !code_show_err
} 
$( document ).ready(code_toggle_err);
</script>
To toggle on/off output_stderr, click <a href="javascript:code_toggle_err()">here</a>.''')

books=pd.read_csv('Books.csv',sep=';',error_bad_lines=False,encoding='latin-1')

books.columns=['ISBN','booktitle','bookAuthor','yearofpulblication','publisher','imageurls','imageurlm','imageurl']

books.head()

users=pd.read_csv('Users.csv',sep=';',encoding='latin-1')

users.columns=['userID','Location','Age']
users.head()

ratings=pd.read_csv('Ratings.csv',sep=';',encoding='latin-1')
ratings.coloumns=['userId','ISBN','bookRating']

print(books.shape)
print(users.shape)
print(ratings.shape)



"""In BOOks we find that urls columns do not seems to be required for analysis thus we can drop the coloumns"""

books.drop(['imageurls','imageurlm','imageurl'],axis=1,inplace=True)
books.head()

books.dtypes

pd.set_option('display.max_colwidth',-1)

books.yearofpulblication.unique()



"""There are some incorrect entries in yearOfPublication. It looks like publisher names ‘DK Publishing Inc’ and ‘Gallimard’ have been incorrectly loaded as yearOfPublication in dataset due to some errors in csv file. Also, some of the values are strings and same years have been entered as numbers at some places. We will make necessary correction for these rows and set the data type for yearOfPublication as int."""

books.loc[books['yearofpulblication']=='DK Publishing Inc']

books.loc[books['ISBN']== '0789466953','yearofpulblication']=2000
books.loc[books['ISBN']== '0789466953','publisher']='DK Publishing Inc'
books.loc[books['ISBN']== '0789466953','bookAuthor']='James Buckley'
books.loc[books['ISBN']== '0789466953','booktitle']='DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)'

books.loc[books['ISBN']== '0789466953','yearofpulblication']=2000
books.loc[books['ISBN']== '0789466953','publisher']='DK Publishing Inc'
books.loc[books['ISBN']== '0789466953','bookAuthor']='Michael Teitelbaum'
books.loc[books['ISBN']== '0789466953','booktitle']='DK Readers: Creating the X-Men, How It All Began (Level 4: Proficient Readers)'

books.loc[books['yearofpulblication']=='Galimard']

books.loc[books.ISBN=='2070426769']

books.loc[books['ISBN']== '2070426769','yearofpulblication']=2003
books.loc[books['ISBN']== '2070426769','publisher']='Gallimard'
books.loc[books['ISBN']== '2070426769','bookAuthor']='Jean-Marie Gustave Le ClÃ?Â©zio'
books.loc[books['ISBN']== '2070426769','booktitle']='Peuple du ciel, suivi de Les Bergers'

books['yearofpulblication']=pd.to_numeric(books['yearofpulblication'],errors='coerce')

print(sorted(books['yearofpulblication'].unique()))

books.loc[(books.yearofpulblication>2006) |(books.yearofpulblication==0),'yearofpulblication']=np.NAN

(books['yearofpulblication'])

books.yearofpulblication.fillna(round(books.yearofpulblication.mean()),inplace=True)
books.yearofpublication=books.yearofpulblication.astype(np.int32)

books

books.isnull().any()

books.isnull().any().sum()

books.loc[books['publisher'].isnull()]

books.loc[(books.ISBN=='193169656X'),'publisher']='other'
books.loc[(books.ISBN=='1931696993'),'publisher']='other'

books.loc[(books.ISBN=='9627982032'),'bookAuthor']='others'



"""Users Dataset
Now we explore users dataset, firstly by checking its shape, first few columns and data types.
"""

print(users.shape)

users.head()

print(sorted(users.Age.unique()))



"""In my view ages below 5 and above 90 do not make much sense, and hence, these are being replaced with NaNs. All the NaNs are then replaced with mean value of Age, and its data type is set as int."""

users.loc[(users['Age']<5)|(users['Age']>90),'Age']=np.NAN

users.Age=users.Age.fillna(users.Age.mean())
users.Age=users.Age.astype(np.int32)

users.head()



"""**Rating Dataset**"""



"""We check the ratings dataset for its shape and first few rows. It reveals that our user-book ratings matrix will be very sparse as actual ratings are quite less as compared to size of ratings matrix (number of users × number of books)."""

ratings.shape

n_users=users.shape[0]
n_books=users.shape[0]
print(n_users*n_books)

ratings.head(10)

"""Now ratings dataset should have userID and ISBN which exist in respective tables, viz. users and books."""

ratings.rename(columns={'User-ID':'userID'})

ratings_new=ratings[ratings.ISBN.isin(books.ISBN)]
ratings_new
ratings_new.rename(columns={'User-ID':'userid'},inplace=True)
ratings_new

ratings_new=ratings_new[ratings_new['userid'].isin(users.userID)]
ratings_new

print(ratings.shape)
print(ratings_new.shape)

"""It is evident that users have rated some books which are not the part of original books"""

sparsity =1-len(ratings_new)/float(n_users*n_books)
print(sparsity)

print("The sparsity of Book Crossing dataset is "+str(sparsity*100)+'%')

"""The explicit ratings represented by 1–10 and implicit ratings represented by 0 will have to be segregated now. We will be using only explicit ratings for building our book recommendation system. Similarly, users are also segregated into those who rated explicitly and those whose implicit behavior was recorded."""

ratings.rename(columns={'Book-Rating':'BookRating'},inplace=True)
ratings_new.rename(columns={'Book-Rating':'BookRating'},inplace=True)
ratings.BookRating.unique()

ratings_explicit=ratings_new[ratings_new.BookRating!=0]
ratings_implicit=ratings_new[ratings_new.BookRating==0]

ratings_explicit

ratings_implicit

user_exp_ratings=users[users.userID.isin(ratings_explicit.userid)]
user_imp_ratings=users[users.userID.isin(ratings_implicit.userid)]

sns.countplot(data=ratings_explicit,x='BookRating')
plt.show()



"""# **Collaborative Filtering based Recommending System**"""



"""To cope up with computing power my machine has and to reduce the dataset size, I am considering users who have rated at least 100 books and books which have at least 2100 ratings."""

count1=ratings_explicit['userid'].value_counts()

count1[count1>=100]

ratings_explicit=ratings_explicit[ratings_explicit['userid'].isin(count1[count1>=100].index)]

counts=ratings_explicit['BookRating'].value_counts()



counts[counts>100].index

ratings_explicit=ratings_explicit[ratings_explicit['BookRating'].isin(counts[counts>=100].index)]

ratings_explicit

rating_matrix=ratings_explicit.pivot(index='userid',columns='ISBN',values='BookRating')

rating_matrix

n_users = rating_matrix.shape[0] #considering only those users who gave explicit ratings
n_books = rating_matrix.shape[1]
print (n_users)
print(n_books)

rating_matrix.fillna(0, inplace = True)
rating_matrix = rating_matrix.astype(np.int32)

rating_matrix.head(5)



global metric,k
k=10
metric='cosine'

"""# **User-based Recommendation System**"""

#This function finds k similar users given the user_id and ratings matrix 
#These similarities are same as obtained via using pairwise_distances
def findksimilarusers(user_id, ratings, metric = metric, k=k):
    similarities=[]
    indices=[]
    model_knn = NearestNeighbors(metric = metric, algorithm = 'brute') 
    model_knn.fit(ratings)
    loc = ratings.index.get_loc(user_id)
    distances, indices = model_knn.kneighbors(ratings.iloc[loc, :].values.reshape(1, -1), n_neighbors = k+1)
    similarities = 1-distances.flatten()
            
    return similarities,indices

#This function predicts rating for specified user-item combination based on user-based approach
def predict_userbased(user_id, item_id, ratings, metric = metric, k=k):
    prediction=0
    user_loc = ratings.index.get_loc(user_id)
    item_loc = ratings.columns.get_loc(item_id)
    similarities, indices=findksimilarusers(user_id, ratings,metric, k) #similar users based on cosine similarity
    mean_rating = ratings.iloc[user_loc,:].mean() #to adjust for zero based indexing
    sum_wt = np.sum(similarities)-1
    product=1
    wtd_sum = 0 
    
    for i in range(0, len(indices.flatten())):
        if indices.flatten()[i] == user_loc:
            continue;
        else: 
            ratings_diff = ratings.iloc[indices.flatten()[i],item_loc]-np.mean(ratings.iloc[indices.flatten()[i],:])
            product = ratings_diff * (similarities[i])
            wtd_sum = wtd_sum + product
    
    #in case of very sparse datasets, using correlation metric for collaborative based approach may give negative ratings
    #which are handled here as below
    if prediction <= 0:
        prediction = 1   
    elif prediction >10:
        prediction = 10
    
    prediction = int(round(mean_rating + (wtd_sum/sum_wt)))
    print ('\nPredicted rating for user {0} -> item {1}: {2}'.format(user_id,item_id,prediction))

    return prediction

predict_userbased(11676,'0001056107',rating_matrix)



"""# **Item-based Recommendation Systems**"""

def findksimilaritems(item_id, ratings, metric=metric, k=k):
    similarities=[]
    indices=[]
    ratings=ratings.T
    loc = ratings.index.get_loc(item_id)
    model_knn = NearestNeighbors(metric = metric, algorithm = 'brute')
    model_knn.fit(ratings)
    
    distances, indices = model_knn.kneighbors(ratings.iloc[loc, :].values.reshape(1, -1), n_neighbors = k+1)
    similarities = 1-distances.flatten()

    return similarities,indices



#This function predicts the rating for specified user-item combination based on item-based approach
def predict_itembased(user_id, item_id, ratings, metric = metric, k=k):
    prediction= wtd_sum =0
    user_loc = ratings.index.get_loc(user_id)
    item_loc = ratings.columns.get_loc(item_id)
    similarities, indices=findksimilaritems(item_id, ratings) #similar users based on correlation coefficients
    sum_wt = np.sum(similarities)-1
    product=1
    for i in range(0, len(indices.flatten())):
        if indices.flatten()[i] == item_loc:
            continue;
        else:
            product = ratings.iloc[user_loc,indices.flatten()[i]] * (similarities[i])
            wtd_sum = wtd_sum + product                              
    prediction = int(round(wtd_sum/sum_wt))
    
    #in case of very sparse datasets, using correlation metric for collaborative based approach may give negative ratings
    #which are handled here as below //code has been validated without the code snippet below, below snippet is to avoid negative
    #predictions which might arise in case of very sparse datasets when using correlation metric
    if prediction <= 0:
        prediction = 1   
    elif prediction >10:
        prediction = 10

    print ('\nPredicted rating for user {0} -> item {1}: {2}'.format(user_id,item_id,prediction))     
    
    return prediction

prediction = predict_itembased(11676,'0001056107',rating_matrix)

@contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:  
            yield
        finally:
            sys.stdout = old_stdout

#This function utilizes above functions to recommend items for item/user based approach and cosine/correlation. 
#Recommendations are made if the predicted rating for an item is >= to 6,and the items have not been rated already
def recommendItem(user_id, ratings, metric=metric):    
    if (user_id not in ratings.index.values) or type(user_id) is not int:
        print ("User id should be a valid integer from this list :\n\n {} ".format(re.sub('[\[\]]', '', np.array_str(rating_matrix.index.values))))
    else:    
        ids = ['Item-based (correlation)','Item-based (cosine)','User-based (correlation)','User-based (cosine)']
        select = widgets.Dropdown(options=ids, value=ids[0],description='Select approach', width='1000px')
        def on_change(change):
            clear_output(wait=True)
            prediction = []            
            if change['type'] == 'change' and change['name'] == 'value':            
                if (select.value == 'Item-based (correlation)') | (select.value == 'User-based (correlation)') :
                    metric = 'correlation'
                else:                       
                    metric = 'cosine'   
                with suppress_stdout():
                    if (select.value == 'Item-based (correlation)') | (select.value == 'Item-based (cosine)'):
                        for i in range(ratings.shape[1]):
                            if (ratings[str(ratings.columns[i])][user_id] !=0): #not rated already
                                prediction.append(predict_itembased(user_id, str(ratings.columns[i]) ,ratings, metric))
                            else:                    
                                prediction.append(-1) #for already rated items
                    else:
                        for i in range(ratings.shape[1]):
                            if (ratings[str(ratings.columns[i])][user_id] !=0): #not rated already
                                prediction.append(predict_userbased(user_id, str(ratings.columns[i]) ,ratings, metric))
                            else:                    
                                prediction.append(-1) #for already rated items
                prediction = pd.Series(prediction)
                prediction = prediction.sort_values(ascending=False)
                recommended = prediction[:10]
                print ("As per {0} approach....Following books are recommended...".format(select.value))
                for i in range(len(recommended)):
                     print ("{0}. {1}".format(i+1,books.booktitle[recommended.index[i]].encode('utf-8')))                        
        select.observe(on_change)
        display(select)

recommendItem(999999,rating_matrix)

recommendItem(2033, rating_matrix)



"""# **Simple Popularity based Recommendation System**"""

ratings_count=pd.DataFrame(ratings_explicit.groupby(['ISBN'])['BookRating'].sum())

ratings_count
top10=ratings_count.sort_values('BookRating',ascending=False).head(10)

top10

print("Following are the Trending Books")
top10.merge(books,left_index=True,right_on='ISBN')



